{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "First.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victory06/TFG/blob/master/First.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP--tUwYCqt-",
        "outputId": "e19b92b9-cf9b-47c5-d635-8475d48fa245"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBHhekXA7UHv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from scipy.io import arff\n",
        "from sklearn.feature_selection import SelectFromModel, VarianceThreshold, SelectKBest\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import Perceptron, Lasso, LogisticRegression, LinearRegression, SGDRegressor, Ridge\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import make_pipeline\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6UuI0H37UH8"
      },
      "source": [
        "\"\"\"\n",
        "Lee las bases de datos y elimina los datos perdidos:\n",
        "    - path: indica el path que se leerá siempre\n",
        "    - three_paths: indica si se quieren leer las tres bases de datos o solo una\n",
        "    - strategy_SI: la estrategia a seguir para quitar los datos perdidos\n",
        "\"\"\"\n",
        "def readData(path, path2, path3, three_paths=False, strategy_SI = \"most_frequent\"):\n",
        "    if(three_paths==False):\n",
        "      data = arff.loadarff(path)\n",
        "      dff = pd.DataFrame(data[0])\n",
        "      dff = dff.apply(LabelEncoder().fit_transform)\n",
        "      df = dff.to_numpy()[:,:].astype('U13')\n",
        "      df = df.astype(float)\n",
        "      imp = SimpleImputer(strategy=strategy_SI)\n",
        "      df = imp.fit_transform(df)\n",
        "    else:\n",
        "      data1 = arff.loadarff(path)\n",
        "      dff1 = pd.DataFrame(data1[0])\n",
        "      dff1 = dff1.apply(LabelEncoder().fit_transform)\n",
        "      df1 = dff1.to_numpy()[:,:].astype('U13')\n",
        "      df1 = df1.astype(float)\n",
        "\n",
        "      data2 = arff.loadarff(path)\n",
        "      dff2 = pd.DataFrame(data2[0])\n",
        "      dff2 = dff2.apply(LabelEncoder().fit_transform)\n",
        "      df2 = dff2.to_numpy()[:,:].astype('U13')\n",
        "      df2 = df2.astype(float)\n",
        "\n",
        "      data3 = arff.loadarff(path)\n",
        "      dff3 = pd.DataFrame(data3[0])\n",
        "      dff3 = dff3.apply(LabelEncoder().fit_transform)\n",
        "      df3 = dff3.to_numpy()[:,:].astype('U13')\n",
        "      df3 = df3.astype(float)\n",
        "\n",
        "      df = np.concatenate((df1,df2,df3),axis=0)\n",
        "      np.random.shuffle(df)\n",
        "      imp = SimpleImputer(strategy=strategy_SI)\n",
        "      df = imp.fit_transform(df)\n",
        "\n",
        "    return df[:, :-1], df[:, -1], df\n",
        "\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHHdkwPN7UH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13883d70-3bd8-4672-932d-a6c3bcf78eb2"
      },
      "source": [
        "x, y, df = readData('/content/drive/MyDrive/TFG/ChildASD/Autism-Child-Data.arff','/content/drive/MyDrive/TFG/AdolescentASD/Autism-Adolescent-Data.arff','/content/drive/MyDrive/TFG/AdultASD/Autism-Adult-Data.arff',three_paths=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.30, random_state=1)\n",
        "df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1., ..., 0., 2., 1.],\n",
              "       [1., 0., 0., ..., 0., 2., 0.],\n",
              "       [0., 0., 1., ..., 0., 2., 1.],\n",
              "       ...,\n",
              "       [0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 2., 1.],\n",
              "       [1., 1., 1., ..., 0., 2., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2pdd09s7UIC"
      },
      "source": [
        "preproc=[(\"var\", VarianceThreshold(0.01)),\n",
        "        (\"standardize\", StandardScaler()),\n",
        "        (\"poly\", PolynomialFeatures(2))]\n",
        "\n",
        "pipe=Pipeline(preproc + [('model', SGDRegressor())])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0tgpcRO7UIE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c351339a-943f-49cf-f9a4-8d7df5fd7638"
      },
      "source": [
        "\n",
        "params_grid=[\n",
        "        {\"model\":[SGDRegressor(max_iter=500)],\n",
        "               \"model__loss\":['huber', 'squared_loss', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
        "               \"model__penalty\":['l1','l2'],\n",
        "               \"model__alpha\":np.logspace(-5,5,5),\n",
        "               \"poly__degree\":[1,2]},\n",
        "        {\"model\":[LinearRegression()],\n",
        "               \"poly__degree\":[1,2]},\n",
        "        {\"model\":[Ridge()],\n",
        "               \"poly__degree\":[1,2],\n",
        "               \"model__alpha\":np.logspace(-5,5,5)},\n",
        "        {\"model\":[Lasso()],\n",
        "               \"poly__degree\":[1,2],\n",
        "               \"model__alpha\":np.logspace(-5,5,5)}]\n",
        "\"\"\"\n",
        "clf = make_pipeline(preproc,GridSearchCV(LogisticRegression(),\n",
        "        param_grid=[\n",
        "        {\"lr\":[LogisticRegression(penalty='l1',max_iter=500)],\n",
        "                \"lr__C\":np.logspace(-2,2,5),\n",
        "                \"lr__solver\":['lbfgs']},\n",
        "       {\"lr\": [RandomForestClassifier(random_state = 1,\n",
        "                                       n_jobs = -1, criterion = 'entropy')],\n",
        "         \"lr__n_estimators\": [100, 200],\n",
        "         \"lr__max_depth\": [6, 8]},\n",
        "        {\"lr\": [SVC(kernel='rbf', gamma='scale', max_iter=1000, degree=2)],\n",
        "               \"lr__C\":np.logspace(-2,2,5)},\n",
        "        {\"lr\": [MLPClassifier(random_state=1, max_iter=1000, hidden_layer_sizes=60)],\n",
        "           \"lr__solver\":['sgd'],\n",
        "           \"lr__activation\":['relu''logistic']}]))\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclf = make_pipeline(preproc,GridSearchCV(LogisticRegression(),\\n        param_grid=[\\n        {\"lr\":[LogisticRegression(penalty=\\'l1\\',max_iter=500)],\\n                \"lr__C\":np.logspace(-2,2,5),\\n                \"lr__solver\":[\\'lbfgs\\']},\\n       {\"lr\": [RandomForestClassifier(random_state = 1,\\n                                       n_jobs = -1, criterion = \\'entropy\\')],\\n         \"lr__n_estimators\": [100, 200],\\n         \"lr__max_depth\": [6, 8]},\\n        {\"lr\": [SVC(kernel=\\'rbf\\', gamma=\\'scale\\', max_iter=1000, degree=2)],\\n               \"lr__C\":np.logspace(-2,2,5)},\\n        {\"lr\": [MLPClassifier(random_state=1, max_iter=1000, hidden_layer_sizes=60)],\\n           \"lr__solver\":[\\'sgd\\'],\\n           \"lr__activation\":[\\'relu\\'\\'logistic\\']}]))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO2b-fKh7UIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61283cba-70bc-4cc3-a004-8e14a523533b"
      },
      "source": [
        "best_model=GridSearchCV(pipe,params_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=1)\n",
        "best_model.fit(x_train,y_train)\n",
        "print(\"Parámetros del mejor clasificador:\\n{}\".format(best_model.best_params_))\n",
        "print(\"Error en CV: {:0.3f}%\".format(100.0 * best_model.best_score_))\n",
        "print(\"Error en training: {:0.3f}%\".format(\n",
        "        100.0 * best_model.score(x_train, y_train)))\n",
        "print(\"Error en test: {:0.3f}%\".format(\n",
        "        100.0 * best_model.score(x_test, y_test)))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 102 candidates, totalling 510 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 216 tasks      | elapsed:    2.9s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parámetros del mejor clasificador:\n",
            "{'model': Lasso(alpha=0.0031622776601683794, copy_X=True, fit_intercept=True,\n",
            "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
            "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False), 'model__alpha': 0.0031622776601683794, 'poly__degree': 2}\n",
            "Error en CV: -4.449%\n",
            "Error en training: -2.810%\n",
            "Error en test: -4.481%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 510 out of 510 | elapsed:    5.2s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaIoUxDe7UIH"
      },
      "source": [
        "clf.fit(x_train,y_train)\n",
        "clf.score(x_text,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}