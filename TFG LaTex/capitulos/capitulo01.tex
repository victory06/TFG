% !TeX root = ../libro.tex
% !TeX encoding = utf8

\chapter{Inferencia estadística y estadística empleada}
En este primer capítulo se va a exponer, justificar y demostrar la inferencia estadística y la estadística empleada en el capítulo de explicación teórica de los modelos \autoref{ch:machine-learning} y en los tests de contraste de hipótesis usados.\\
\section{El espacio muestral. Eventos}
Imaginemos que repetimos un mismo experimento aleatorio, por ejemplo lanzar una moneda, numerosas veces. No siempre esperamos obtener el mismo resultado, algunos resultados serán más probables que otros. Cada ejecución del experimento produce exactamente un conjunto de posibles resultados, nunca obtenemos dos o más resultados de un solo experimento y nunca obtenemos un experimento sin resultados. Así, podemos calcular la frecuencia de que cada resultado aparezca.
\begin{definicion}
El \textbf{espacio muestral} $\Omega$ es el conjunto de todos los posibles resultados de un experimento aleatorio.
\end{definicion} 
Supongamos ahora que ejecutamos un experimento y obtenemos un resultado. Podemos ver si está en un conjunto particular de posibles resultados así que deberíamos ser capaces de predecir la probabilidad de un conjunto de resultados, por ejemplo, la probabilidad de obtener un número par al tirar un dado. Esto nos lleva a la definición de evento.
\begin{definicion}
Un \textbf{evento} es un conjunto de resultados, un subconjunto del espacio muestral
\end{definicion}
\cite{forsyth2018probability}\\
Los eventos tienen las siguientes propiedades básicas:\\
\begin{itemize}
 \item La probabilidad de cualquier evento está entre cero y uno. \[0 \leq P(\mathcal{A}) \leq 1\] para cualquier evento $\mathcal{A}$
 \item Todos los experimentos tienen un resultado. Esto quiere decir que \[ P(\Omega)=1 \]
 \item La probabilidad de eventos disjuntos es aditiva. Sea $\mathcal{A}_{i}$ una colección de eventos tales que $\mathcal{A}_{i} \cap \mathcal{A}_{j}=\emptyset$ cuando $i \neq j$, lo que quiere decir que no hay ningún resultado que aparezca en más de un $\mathcal{A}_{i}$. Así, se tiene que
\[ P(\cup_{i}\mathcal{A}_{i})= \sum_{i}P(\mathcal{A}_{i}) \]
\end{itemize}
Denotamos como $\mathcal{A}^{c}$ al evento complementario de $\mathcal{A}$.
\begin{proposicion}
$P(\mathcal{A}^{c})=1-P(\mathcal{A})$
\end{proposicion}
\begin{proof}
$\mathcal{A}^{c}$ y $\mathcal{A}$ son eventos disjuntos así que $P(\mathcal{A}^{c} \cup \mathcal{A})=P(\mathcal{A}^{c})+P(\mathcal{A})=P(\Omega)=1$.
\end{proof}
\begin{proposicion}
$P(\emptyset)=0$
\end{proposicion}
\begin{proof}
$P(\emptyset)=P(\Omega^{c})=P(\Omega - \Omega)=1-P(\Omega)=1-1=0$
\end{proof}
\begin{proposicion}
Para cualquiera dos eventos $\mathcal{A}$ y $\mathcal{B}$, $P(\mathcal{A}-\mathcal{B})=P(\mathcal{A})-P(\mathcal{A} \cap \mathcal{B})$
\end{proposicion}
\begin{proof}
$\mathcal{A}-\mathcal{B}$ es disjunto de $\mathcal{A} \cap \mathcal{B}$, y $(\mathcal{A}-\mathcal{B}) \cup (\mathcal{A} \cap \mathcal{B})=\mathcal{A}$. Esto significa que $P(\mathcal{A}-\mathcal{B})+P(\mathcal{A} \cap \mathcal{B})=P(\mathcal{A})$
\end{proof}
\begin{proposicion}
Para cualquiera dos eventos $\mathcal{A}$ y $\mathcal{B}$, $P(\mathcal{A} \cup \mathcal{B})=P(\mathcal{A}) + P(\mathcal{B})-P(\mathcal{A} \cap \mathcal{B})$
\end{proposicion}
\begin{proof}
$P(\mathcal{A} \cup \mathcal{B})=P(\mathcal{A} \cup (\mathcal{B} \cup \mathcal{A}^{c}))=P(\mathcal{A})+P((\mathcal{B} \cup \mathcal{A}^{c}))$. Ahora $\mathcal{B}=(\mathcal{B} \cup \mathcal{A})\cup (\mathcal{B} \cap \mathcal{A}^{c})$. Además, $(\mathcal{B} \cap \mathcal{A})$ es disjunto de $(\mathcal{B} \cap \mathcal{A}^{c})$ así que tenemos que $P(\mathcal{B})=P((\mathcal{B} \cup \mathcal{A})) + P((\mathcal{B} \cap \mathcal{A}^{c}))$. Esto significa que $P(\mathcal{A} \cup \mathcal{B})=P(\mathcal{A})+P((\mathcal{B} \cap \mathcal{A}^{c}))=P(\mathcal{A})+P(\mathcal{B})-P((\mathcal{B} \cup \mathcal{A}))$.
\end{proof}
\cite{forsyth2018probability}\\
Algunos resultados experimentales no afectan a otros, por ejemplo, en dos tiradas de una moneda el resultado de la primera no afecta al de la segunda. Nos referimos a eventos con esta propiedad como \textbf{independientes}.
\begin{definicion}
Dos eventos $\mathcal{A}$ y $\mathcal{B}$ son independientes si y solo si
\[ P(\mathcal{A} \cap \mathcal{B})=P(\mathcal{A})P(\mathcal{B}) \]
\end{definicion}
En cambio, si no son independientes entonces sabiendo que ha ocurrido uno, puede tener un efecto significativo en la probabilidad de que ocurra el otro. Por ello, hacemos la siguiente definición.
\begin{definicion}
Definimos la \textbf{probabilidad condicional} de $\mathcal{B}$ condicionada a $\mathcal{A}$ como la probabilidad de que ocurra $\mathcal{B}$ cuando ha ocurrido $\mathcal{A}$. La escribimos como $P(\mathcal{B}\vert \mathcal{A})=\frac{P(\mathcal{A} \cap \mathcal{B})}{P(\mathcal{A})}$.
\end{definicion}
Otra forma de escribir la probabilidad consicionada es:
\[ P(\mathcal{B}\vert \mathcal{A})P(\mathcal{A})=P(\mathcal{B} \cap \mathcal{A}) \]
y ahora como $\mathcal{B} \cap \mathcal{A}=\mathcal{A} \cap \mathcal{B}$, tenemos que:
\[ P(\mathcal{B}\vert \mathcal{A})=\frac{P(\mathcal{A}\vert \mathcal{B})P(\mathcal{B})}{P(\mathcal{A})} \]
\cite{forsyth2018probability}
\section{Variables aleatorias}
\begin{definicion}
\textbf{(Variable Aleatoria Discreta) } Dado un espacio muestral $\Omega$, un conjunto de eventos $\mathcal{F}$, una función de probabilidad $P$ y un conjunto numerable $D$ de número reales, una variable aleatoria discreta es una función con dominio $\Omega$ y rango $D$.
\end{definicion}
La probabilidad de que una variable aleatoria $X$ tome el valor $x$ viene dado por $P(X=x)$, definida como:
\begin{definicion}
\textbf{Distribución de probabilidad de una variable aleatoria discreta}. Es un conjunto de números $P(X=x)$ para cada valor $x$ que $X$ puede tomar. La distribución toma el valor 0 en el resto de valores y es no negativa. Se conoce a veces como la \textbf{función masa de probabilidad}.
\end{definicion}
Ahora pasemos a definir la distribución de probabilidad conjunta para dos variables aleatorias.
\begin{definicion}
\textbf{Distribución de probabilidad conjunta de dos variables aleatorias discretas}. Sean $X$ e $Y$ dos variables aleatorias discretas, la probabilidad de que $X$ tome el valor $x$ e $Y$ tome el valor $y$ es $P((X=x) \cap (Y=y))$ y se escribe como $P(x,y)$.
\end{definicion}
\begin{definicion}
Sea $P(x,y)$ la distribución de probabilidad conjunta de dos variables aleatorias $X$ e $Y$, entonces 
\[ P(x)=\sum_{y}P(x,y)=P(X=x) \]
se refiere a la \textbf{distribución de probabilidad marginal} de $X$.
\end{definicion}
El siguiente concepto tiene que ver con la independencia, en concreto de dos variables aleatorias.
\begin{definicion}
Dos variables aleatorias $X$ e $Y$ son \textbf{independientes} si los eventos $X=x$ e $Y=y$ son independientes para todos los valores $x$ e $y$. Esto significa que 
\[ P(x,y)=P(x)P(y) \]
\end{definicion}
Para las variables aleatorias continuas, la definición es análoga a la discreta excepto que el espacio $D$ es continuo. Damos la siguiente interpretación  de la función de densidad.\\
Sea $p(x)$ una función de densias para una variable aleatoria continua $X$, asumimos que $dx$ es un intervalo infinitesimal, entonces:
\[ p(x)dx=P(\textnormal{evento en el que X toma el valor del rango [x,x+dx]}) \]
\cite{forsyth2018probability}
Pasamos ahora a hablar sobre la esperanza.
\begin{definicion}
\textbf{Esperanza}. Dada una variable aleatoria discreta $X$ que toma valores en un conjunto $D$ y con una distribución de probabilidad $P$, definimos la esperanza como
\[ E[X]=\sum_{x \in D} xP(X=x) \]
Si tenemos una función $f$ que lleva la variable aleatoria $X$ a un conjunto de números $D_{f}$ entonces $f(X)$ también es una variable aleatoria que escribimos como $F$. La esperanza entonces de esta variable se escribe como
\[ E[f]=\sum_{x \in D_{f}} uP(F=u)=\sum_{x \in D}P(X=x) \]
que se llama a veces la 'esperanza de $f$'.
\end{definicion}
En su versión continua, la esperanza se define como sigue.
\begin{definicion}
Dada una variable continua $X$ que toma valores en el conjunto $D$ y tiene una distribución $P$, definimos la esperanza como
\[ E[X]=\int_{x \in D} xp(x)dx \]
Y si tenemos una función $f$ que lleva $X$ a un conjunto de números $D_{f}$, entonces $f(X)$ es una variable aleatoria continua también, que se escribe como $F$. La esperanza de esta variable aleatoria es 
\[ E[f]=\int_{x \in D} f(x)p(x)dx \]
que se llama a veces la 'esperanza de $f$'.
\end{definicion}
La media o esperanza de una variable aleatoria $X$ es $E[X]$ y sus propiedades más importantes son:
\begin{itemize}
 \item $E[0]=0$
 \item para cualquier constante $k$, $E[kf]=kE[f]$
 \item $E[f+g]=E[f]+E[g]$
\end{itemize}
que son consecuencia de las propiedades de la integral y el sumatorio. Y una última propiedad importante, que necesita de demostración, se muestra a continuación:
\begin{proposicion}
Si $X$ e $Y$ son variables aleatorias independientes, entonces $E[XY]=E[X]E[Y]$
\end{proposicion}
\begin{proof}
Tenemos que:
\[ E[XY]=\sum_{(x,y) \in D_{x} \times D_{y}} xyP(X=x,Y=y) = \sum_{x \in D_{x}} \sum_{y \in D_{y}} (xyP(X=x,Y=y)) \]
y por ser X e Y independientes, tenemos que
\[ \sum_{x \in D_{x}} \sum_{y \in D_{y}} (xyP(X=x)P(Y=y)) = \left( \sum_{x \in D_{x}} xP(X=x) \right) \left( \sum_{y \in D_{y}} yP(Y=y) \right) \]
\[ =E[X]E[Y] \]
\end{proof}
se procede de forma análoga para el caso continuo.\\
Ahora definimos la varianza.
\begin{definicion}
\textbf{Varianza}. La varianza de una variable aleatoria $X$ es
\[ var[X]=E[(X-E[X])^{2}] \]
\end{definicion}
Las propiedades principales de la varianza son:
\begin{itemize}
 \item Para cualquier constante $k$, $var[k]=0$;
 \item $var[X] \geq 0$;
 \item $var[kX]=k^{2}var[X]$;
 \item Si $X$ e $Y$ son independientes, entonces $var[X+Y]=var[X]+var[Y]$;
\end{itemize}
donde las tres primeras vienen directamente de la definición y la cuarta vamos a probarla a continuación.
\begin{proof}
(Prueba de la cuarta propiedad de la varianza). Si $X$ e $Y$ son independientes, entonces $(X-E[X])$ e $(Y-E[Y])$ también lo son. Tenemos así que:
\[ var[X+Y]=E[(X+Y-E[X+Y])^{2}]=E[(X-E[X]+Y-E[Y])^{2}]= \]
\[ =var[X]+var[Y]+2E[(X-E[X])(Y-E[Y])]=var[X]+var[Y]+2E[X-E[X]]E[Y-E[Y]] \]
\[ =var[X]+var[Y] \]
\end{proof}
Para terminar esta sección de conceptos básicos, vamos a definir lo que son las muestras aleatorias independientes e idénticamente distribuidas.\\
Observar un valor de una variable aleatoria se suele llamar \textbf{ensayo}. El valor resultante se suele llamar \textbf{muestra} de una variable aleatoria (o de su distribución de probabilidad) o a veces \textbf{realización}. Con esto, llegamos a la definición:
\begin{definicion}
Supongamos que tenemos un conjunto de variables aleatorias $X_{i}$ que son independientes y tienen la misma distribución $P(X)$. Entonces nos referimos a este conjunto como \textbf{muestras aleatorias independientes e idénticamente distribuidas (muestras iid)} o \textbf{muestra aleatoria simple (m.a.s.)} de una variable $X$ con distribución $P(X)$.
\end{definicion}
\cite{forsyth2018probability}
\section{Estimadores máximo verosímiles}\label{st:emv}
Sean $X_{1},...,X_{n}$ muestras independientes e idénticamente distribuidas de una población con función masa de probabilidad o función de densidad $f(x \vert \theta_{1},...,\theta_{k})$, la función de verosimilitud se define como
\[ L(\theta \vert x)=L(\theta_{1},...,\theta_{k} \vert x_{1},...,x_{n})=\prod_{i=1}^{n} f(x_{1} \vert \theta_{1},...,\theta_{k}) \]
\begin{definicion}
Para cada realización muestral x, sea $\hat{\theta}(x)$ un valor paramétrico donde $L(\theta \vert x)$ alcanza su máximo como función de $\theta$, con x fija. Un \textbf{\textit{Estimador Máximo Verosímil}} (EMV) del parámetro $\theta$ basado en la muestra X es $\hat{\theta}(x)$. \cite{garthwaite2002statistical} \end{definicion}
Por la forma en la que está construído, el EMV coincide en rango con el parámetro $\theta$.\\
Una propiedad muy útil de los estimadores máximo verosímiles es lo que se conoce como \textit{propiedad de invarianza de los estimadores máximo verosímiles}, que viene a decir, informalmente, que si tenemos una distribución dependiente de un parámetro $\theta$ pero nos interesa encontrar una estimación de alguna función suya, $\tau(\theta)$ se puede encontrar el EMV de $\theta$, $\hat{\theta}$, y el EMV de $\tau(\theta)$ será $\tau(\hat{\theta})$. Hay, sin embargo, algunos problemas técnicos antes de formalizar la noción de invarianza de los EMV. Si la asignación $\theta \rightarrow \tau(\theta)$ es uno a uno (para cada valor de $\theta$ hay un único valor de $\tau(\theta)$), no hay problema pues si tomamos $\eta = \tau(\theta)$, entonces la inversa de la función $\tau^{-1}(\eta)=\theta$ está bien definida y la función de verosimilitud de $\tau(\theta)$ se escribe en función de $\eta$ dada por: \[ L^{*}(\eta \vert x) = \prod_{i=1}^{n}f(x_{i} \vert \tau^{-1}(\eta)) = L(\tau^{-1}(\eta) \vert x) \]
y
\[ \sup_{\eta}L^{*}(\eta \vert x) = \sup_{\eta}L(\tau^{-1}(\eta) \vert x) = \sup_{\theta}L(\theta \vert x) \]
Así, el máximo de $L^{*}(\eta \vert x) $ es alcanzado en $\eta = \tau(\theta) = \tau(\hat{\theta})$ mostrando que el EMV de $\tau(\theta)$ es $\tau(\hat{\theta})$.\\
Ahora bien, si la asignación no es uno a uno, necesitamos una definición más general de  la función de verosimilitud $\tau(\\theta)$ y un teorema más general, pues para un valor dado $\eta$ puede haber más de un valor $\theta$ que satisfaga $\tau(\\theta) = \eta$.\\
Para ello, usamos para $\tau(\theta)$ la \textit{función de verosimilitud inducida} dada por \[ L^{*}(\eta \vert x) = \sup_{\lbrace \theta:\tau(\theta)=\eta \rbrace} L(\theta \vert x) \]
\begin{teorema}
\textbf{(Invarianza de los EMV)} Si $\hat{\theta}$ es el EMV de $\theta$  entonces para cualquier función $\tau(\theta)$ el EMV de $\tau(\theta)$ es $\tau(\hat{\theta})$
\end{teorema}
\begin{proof}
Sea $\hat{\eta}$ el valor que maximiza $L^{*}(\eta \vert x)$. Debemos probar que $L^{*}(\hat{\eta} \vert x) = L^{*}(\tau(\theta) \vert x)$. Ahora, como se puede deducir de la fórmula anterior, el máximo de $L$ y $L^{*}$ coinciden así que tenemos: 
\[ L^{*}(\hat{\eta} \vert x) = \sup_{\eta} \sup_{\lbrace \theta:\tau(\theta)=\eta \rbrace} L(\theta \vert x)= \quad \quad \textnormal{ (definición de } L^{*} \textnormal{)} \]
\[ =\sup_{\theta}L(\theta \vert x) = \quad \quad \quad \quad \]
\[\quad \quad \quad \quad  = L(\hat{\theta} \vert x) \quad \quad \quad \textnormal{(definición de } \hat{\theta} \textnormal{)} \]
donde la segunda igualdad se da ya que la maximización iterada es igual que la maximización sin condiciones sobre $\theta$, que es alcanzada en $\hat{\theta}$. Además
\[ L(\theta \vert x) = \sup_{\lbrace \theta:\tau(\theta)=\tau(\hat{\theta}) \rbrace} L(\theta \vert x)= \quad \quad \textnormal{(} \hat{\theta} \textnormal{ es el EMV)} \]
\[ =L^{*}(\tau(\hat{\theta}) \vert x) \quad \quad \textnormal{(definición de } L^{*} \textnormal{)} \]
Por lo que la cadena de igualdades muestra que $L^{*}(\hat{\eta} \vert x) = L^{*}(\tau(\theta) \vert x)$ y que $\tau(\hat{\theta})$ es el EMV de $\tau(\theta)$.
\end{proof}
Este teorema tampoco excluye la posibilidad de que $\theta$ sea un vector: si el EMV de $\theta=(\theta_{1},...,\theta_{k})$ es $\hat{\theta}=(\hat{\theta_{1}},...,\hat{\theta_{k}})$ entonces para cualquier función $\tau$ del vector, el EMV de $\tau(\theta_{1},...,\theta_{k})$ es $\tau(\hat{\theta_{1}},...,\hat{\theta_{k}})$. \cite{garthwaite2002statistical}
\section{Desigualdad de Chebyshev}\label{st:Cheby}
Antes de probarla, necesitamos probar la siguiente proposición.
\begin{proposicion}
\textbf{Desigualdad de Markov}. Para una variable aleatoria $X$ y un número real $a>0$,
\[ P(\vert X \vert \geq a) \leq \frac{E \left[ \vert X \vert \right]}{a} \]
\end{proposicion}
\begin{proof}
Para todo $a>0$, $aI_{[ \vert X \vert  \geq a]}(X) \leq \vert X \vert$, así que se tiene que $E[aI_{[ \vert X \vert  \geq a]}] \leq E[\vert X \vert]$ y por las propedades de la esperanza, tenemos:
\[ E[aI_{[ \vert X \vert  \geq a]}] = aE[I_{[ \vert X \vert  \geq a]}] = aP(\vert X \vert \geq a) \] y tenemos así que
\[ aP(\vert X \vert \geq a) \leq E \left[ \vert X \vert \right] \]
y se puede despejar la a pues es mayor que 0, concluyendo la demostración.
\end{proof}
Ahora sí estamos en condiciones de probar la desigualdad de Chebyshev.
\begin{proposicion}
\textbf{Desigualdad de Chebyshev}. Para una variable aleatoria $X$ y $a>0$,
\[ P(\vert X - E[X] \vert \geq a) \leq \frac{Var[X]}{a^{2}} \]
\end{proposicion}
\begin{proof}
Escribimos como $U$ a la variable aleatoria $(X - E[X])^{2}$. La desigualdad de Markov nos dice que, para $w>0$, $P(\vert U \vert \geq w) \leq \frac{E[\vert U \vert]}{w}$.\\
Si establecemos $w=a^{2}$, $P(\vert U \vert \geq w) = P(\vert X-E[X] \vert \geq a)$ y tenemos que:
\[ P(\vert X-E[X] \vert \geq a) \leq \frac{E[\vert U \vert]}{w}= \frac{Var[X]}{a^{2}} \]
\end{proof}
\cite{forsyth2018probability}

\endinput
%------------------------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%------------------------------------------------------------------------------------




















