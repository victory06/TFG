% !TeX root = ../libro.tex
% !TeX encoding = utf8

\chapter{Estadística Multivariante empleada}
\section{Primeras nociones}
Rescatamos el concepto de \textbf{distribución conjunta} del capítulo anterior que se define involucrando la función de distribución:
\[ R(x,y)=P(X \leq x, Y \leq y) \]
para todo par de números reales $(x,y)$. Nos interesan en concreto los casos donde $F$ es absolutamente continua, lo que quiere decir que la derivada parcial siguiente existe casi en todas partes (c.t.p.) (es decir no existe para un conjunto de medida nula):
\[ \frac{\partial^{2} F(x,y)}{\partial x \partial y}=f(x,y) \]
y 
\[ F(x,y) = \int_{-\infty}^{y} \int_{-\infty}^{x} f(u,v) du dv   \]
donde $f$ indica la \textbf{función de densidad}.\\
Estas definiciones pueden extrapolarse al caso de $p$ variables aleatorias $X_{1},X_{2},...,X_{p}$. La función de distribución es
\[ F(x_{1},...,x_{p})=P(X_{1} \leq x_{1},...,X_{p} \leq x_{p}) \]
para cualquier conjunto de números reales $x_{1},...,x_{p}$. La función de densidad absolutamente continua (c.t.p.) es
\[ \frac{\partial^{p} F(x_{1},...,x_{p}}{\partial x_{1},...,\partial x_{p}} \]
y 
\[ F(x_{1},...,x_{p})= \int_{-\infty}^{x_{p}} \dotsb \int_{-\infty}^{x_{1}} f(u_{1},...,u_{p})du_{1} \dotsb du_{p} \]
Los \textbf{momentos conjuntos} se definen como
\[ \mathcal{E} X_{1}^{h_{1}} \dotsb X_{p}^{h_{p}} = \int_{-\infty}^{\infty} \dotsb \int_{-\infty}^{\infty} x_{1}^{h_{1}} \dotsb x_{p}^{h_{p}} f(x_{1},...,x_{p})dx_{1} \dotsb dx_{p} \]
La \textbf{distribución marginal} de $r$ variables aleatorias con $r<p$ es:
\[ P(X_{1} \leq x_{1},...,X_{r} \leq x_{r})= P(X_{1} \leq x_{1},...,X_{r} \leq x_{r}, X_{r+1} \leq \infty,...,X_{p} \leq \infty)= \]
\[ =F(x_{1},...,x_{r},\infty,...,\infty) \]
y la \textbf{densidad marginal} de $X_{1},...,X_{r}$ es
\[ \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x_{1},...,x_{r},u_{r+1},...,u_{p})du_{r+1} \dotsb du_{p} \]
Los momentos conjuntos pueden obtenerse de la distribución marginal, por ejemplo:
\[ \mathcal{E} X_{1}^{h_{1}} \dotsb X_{r}^{h_{r}} = \mathcal{E} X_{1}^{h_{1}} \dotsb X_{r}^{h_{r}} X_{r+1}^{0} \dotsb X_{p}^{0} = \int_{-\infty}^{\infty} \dotsb \int_{-\infty}^{\infty} x_{1}^{h_{1}} \dotsb x_{r}^{h_{r}} f(x_{1},...,x_{p})dx_{1} \dotsb dx_{p}= \]
\[ = \int_{-\infty}^{\infty} \dotsb \int_{-\infty}^{\infty} x_{1}^{h_{1}} \dotsb x_{r}^{h_{r}} · \left[ \int_{-\infty}^{\infty} \dotsb \int_{-\infty}^{\infty} f(x_{1},...,x_{p})dx_{r+1} \dotsb dx_{p} \right] dx_{1} \dotsb dx_{r} \]
Si la función de distribución de $X_{1},...,X_{p}$ es $F(x_{1},...,x_{p})$, el conjunto de variables aleatorias se dice que es \textbf{mutuamente independientes} si
\[ F(x_{1},...,x_{p})=F_{1}(x_{1}) \dotsb F_{p}(x_{p}) \textnormal{,} \]
donde $F_{i}(x_{i})$ es la función de distribución marginal de $X_{i}$, i=1,...,p. El conjunto $X_{1},...,X_{r}$ se dice que es \textbf{independiente} de $X_{r+1},...,X_{p}$ si
\[ F(x_{1},...,x_{p})=F(x_{1},...,x_{r},\infty,...,\infty) F(\infty,...,\infty, x_{r+1},...,x_{p}) \textnormal{.}\]
Un resultado de la independencia es que los momentos conjuntos se multiplican. Por ejemplo, si tenemos que $X_{1},...,X_{p}$ son mutamente independientes, entonces
\[ \mathcal{E} X_{1}^{h_{1}} \dotsb X_{p}^{h_{p}} = \int_{-\infty}^{\infty} \dotsb \int_{-\infty}^{\infty} x_{1}^{h_{1}} \dotsb x_{p}^{h_{p}} f(x_{1},...,x_{p})dx_{1} \dotsb dx_{p}= \]
\[ = \prod_{i=1}^{p} \int_{-\infty}^{\infty} x_{i}^{h_{i}} f_{i}(x_{i}) dx_{i} = \prod_{i=1}^{p} \lbrace \mathcal{E} X_{i}^{h_{i}} \rbrace \]
Si la función de distribución de $X_{1},...,X_{p}$ es $F(x_{1},...,x_{p})$, la densidad condicional de $X_{1},...,X_{r}$ dados $X_{r+1}=x_{r+1},...,X_{p}=x_{p}$, es
\[ \frac{f(x_{1},...,x_{p})}{\int_{-\infty}^{\infty} \dotsb \int_{-\infty}^{\infty} f(u_{1},...,r_{r},x_{r+1},...,x_{p})du_{1} \dotsb du_{r}} \]
Por último, vamos a hablar de la transformación de variables.\\
Sea $F(x_{1},...,x_{p})$ la función de densidad de $X_{1},...,X_{p}$. Consideramos las $p$ funciones reales $y_{i}=y_{i}(x_{1},...,x_{p})$ con $i=1,...,p$.\\
Asumimos que la transformación del espacio de x al de y es una a una; la transformación inversa es $x_{i}=x_{i}(y_{1},...,y_{p})$ con $i=1,...,p$.
Sean las variables aleatorias $Y_{1},...,Y_{p}$ definidas como $Y_{i}=y_{i}(X_{1},...,X_{p})$ con $i=1,...,p$. La función de densidad de $Y_{1},...,Y_{p}$ es
\[ g(y_{1},...,y_{p})=f[x_{1}(y_{1},...,y_{p}),...,x_{p}(y_{1},...,y_{p})]J(y_{1},...,y_{p´}), \]
donde $J(y_{1},...,y_{p})$ es el Jacobiano
\begin{equation}
		\renewcommand\arraystretch{2}
        J(y_{1},...,y_{p}) = mod \begin{vmatrix}
            \frac{\partial x_{1}}{\partial y_{1}} & \frac{\partial x_{1}}{\partial y_{2}} & \cdots & \frac{\partial x_{1}}{\partial y_{p}} \\
            \frac{\partial x_{2}}{\partial y_{1}} &\frac{\partial x_{2}}{\partial y_{2}} & \cdots & \frac{\partial x_{2}}{\partial y_{p}} \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial x_{p}}{\partial y_{1}} & \frac{\partial x_{p}}{\partial y_{2}} & \cdots & \frac{\partial x_{p}}{\partial y_{p}} \\
        \end{vmatrix}
    \end{equation}
Donde asumimos que la derivada existe y donde 'mod' se refiere al módulo del determinante. \\
\cite{anderson1958introduction}
\section{Propiedades de matrices definidas positivas}
La forma cuadrática de una matriz la definimos como 
\[x^TAx=\sum_{i,j=1}^p a_{ij}x_ix_j, \]
donde $x^T$ es el vector $(x_1,...,x_p)$ y $A=(a_{ij})$ es una matriz simétrica. Se dice que $A$ y la forma cuadrática son \textit{semidefinidas positivas} si $x^TAx \geq 0$ para todo $x$. Si $x^TAx > 0$ para todo $x \neq 0$ entonces $A$ y la forma cuadrática se llaman \textit{definidas positivas} que implica que la matriz es simétrica.
\begin{teorema}\label{teo1}
Si $C$ es una matriz cuadrada de $p$ filas y columnas definida positiva, y si $B$ una matriz con $p$ filas y $q$ columnas, $q \leq p$, es de rango $q$, entonces $B^TCB$ es definida positiva
\end{teorema}
\begin{proof}
Dado un vector $y\neq0$, sea $x=BY$. Como $B$ es de rango $q$, $By=x\neq 0$. Entonces
\[ y^T(B^TCB)y=(By)^TC(By)=x^TCx>0 \]
y la prueba concluye observando que  $B^TCB$ es simétrica. Notar que sólo se cumple cuando $B$ es de rango $q$ pues en otro caso, existiría $y \neq 0$ tal que $By=0$.
\end{proof}
\cite{anderson1958introduction}\\
La última afirmación de la demostración anterior nos lleva al siguiente corolario:
\begin{corolario}
Si $C$ es definida positiva y $B$ es no singular, entonces $B^TCB$ es definida positiva.
\end{corolario}
\begin{corolario}
Si $C$ es definida positiva entonces $C^{-1}$ es definida positiva.
\end{corolario}
\begin{proof}
$C$ debe ser no singular. Si fuera singular, ocurriría que $Cx=0$ para algún $x \neq 0$, entonces $x^TCx=0$ lo que contradice la hipótesis de que $c$ es definida positiva. Sea $B$ en \autoref{teo1} igual a $C^{-1}$. Entonces $B^TCB=(C^{-1})^TCC^{-1}=(C^{-1})^T$. Transponiendo $CC^{-1}=I$, tenemos que $(C^{-1})^TC^T=(C^{-1})^TC=I$. Por lo que $C^{-1}=(C^{-1})^T$.
\end{proof}

\begin{corolario}
La matriz $q \times q$ formada por la eliminación de $p-q$ filas de una matriz definida positiva $C$ y las correspondientes $p-q$ columnas de $C$ es definida positiva.
\end{corolario}
\begin{proof}
Se deduce del \autoref{teo1} formando $B$ a través de coger la identidad $p \times p$ y borrando las columnas correspondientes a las borradas de $C$.
\end{proof}

\begin{teorema}\label{teo2}
Si $A$ es no singular, existe una matriz triangular inferior no singular $F$ tal que $FA=A^*$ es una matriz triangular superior no singular.
\end{teorema}
\begin{proof}
Sea $A=A_1$. Se define recursivamente $A_G=(a_{ij}^{(g)})=F_{g-1}A_{g-1},g=2,...,p$, donde $F_{g-1}=(f_{ij}^{(g-1)})$ tiene los elementos
\[ f_{jj}^{(g-1)}=1,\textnormal{   } j=1,...,p, \]
\[ f_{i,g-1}^{(g-1)} = \frac{a_{i,g-1}^{(g-1)} }{a_{g-1,g-1}^{(g-1)}}, \textnormal{   } i=g,...,p \]
\[ f_{ij}^{(g-1)}=0, \textnormal{en otro caso} \]
Entonces
\[ a_{ij}^{(g)}=0, \textnormal{     } i=j+1,...,p, j=1,...,g-1, \]
\[ a_{ij}^{(g)}=a_{ij}^{(g-1)}, \textnormal{     } i=1,...,g-1, j=1,...,p, \]
\[ a_{ij}^{(g)}=a_{ij}^{(g-1)} + f_{i,g-1}^{(g-1)} a_{g-1,j}^{(g-1)}=a_{ij}^{(g-1)} - \frac{a_{i,g-1}^{(g-1)} a_{g-1,j}^{(g-1)}}{a_{g-1,g-1}^{(g-1)}}, i,j=g,...,p. \]
Y vemos que $F=F_{p-1},...,F_1$ es triangular inferior y los elementos de $A_g$ en las primeras $g-1$ columnas bajo la diagonal son $0$; en particular, $A^*=FA$ es triangular superior. De $\vert A \vert \neq 0$ y $\vert F_{g-1} \vert=1$, tenemos que $\vert A_{g-1} \vert \neq 0$. Por tanto, $a_{11}^{(1)},...,a_{g-2,g-2}^{(g-2)}$ son diferentews de $0$ y las últimas $p-g$ columnas de $A_{g-1}$ pueden ser numeradas para que $a_{g-1,g-1}^{(g-1)} \neq 0$; entonces $f_{i,g-1}^{(g-1)}$ está bien definida.
\end{proof}



\begin{corolario}
Si $A$ es definida positiva, existe una matriz triangular inferior $G$ tal que $GAG^T=I$
\end{corolario}
\begin{proof}

\end{proof}
\section{Distribución normal multivariante}
La función de densidad univariante puede escribirse como
\[ ke^{-\frac{1}{2}\alpha(x-\beta)^{2}}=ke^{-\frac{1}{2}(x-\beta)\alpha(x-\beta)} \]
donde $\alpha$ es positivo y $k$ es elegida de forma que la función de distribución integrada el todo el eje x sea la unidad. La función de densidad de una distribución normal de $X_{1},...,X_{p}$ tiene una forma análoga. La variable escalar $x$ es sutituida por un vector $x=(x_{1},...,x_{p})^{T}$, el escalar $\beta$ por un vector $b=(b_{1},...,b_p)^T$ y la constante positiva $\alpha$ por una matriz definida positiva (simétrica)
\begin{equation}
A=\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1p}\\
a_{21} & a_{22} & \cdots & a_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
a_{p1} & a_{p2} & \cdots & a_{pp}
\end{pmatrix}
\end{equation} 
El cuadrado $\alpha(x-\beta)^2=(x-\beta)\alpha(x-\beta)$ es reemplazado por la forma cuadrática
\[ (x-b)^T A (x-b) = \sum_{i,j=1}^{p}a_{ij}(x_i-b_i)(x_j-b_j). \]
Por tanto, la función de densidad de una normal p-variante es
\[ f(x_1,...,x_p)=Ke^{-\frac{1}{2}(x-b)^T A (x-b)} ,\]
donde K>0 es elegida de forma que la integral sobre al espacio euclídeo p-dimensional de $x_1,...,x_p$ sea la unidad. Vamos a determinar esa K, evaluando
\[ K^* = \int_{-\infty}^{\infty} \dotsb \int_{-\infty}^{\infty} e^{-\frac{1}{2}(x-b)^T A (x-b)} dx_p \dotsb dx_1 \]
Para ello, vamos a usar el corolario

\cite{anderson1958introduction}
























\endinput
%------------------------------------------------------------------------------------
% FIN DEL CAPÍTULO. 
%------------------------------------------------------------------------------------
